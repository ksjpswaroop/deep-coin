{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original source: https://gist.github.com/jkleint/1d878d0401b28b281eb75016ed29f2ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return: \n",
    "returns a Keras Model for predicting the next value in a timeseries given a fixed-size lookback window of previous values.\n",
    "The model can handle multiple input timeseries (`nb_input_series`) and multiple prediction targets (`nb_outputs`).\n",
    "\n",
    "## Parameters:\n",
    "#### window_size: \n",
    "The number of previous timeseries values to use as input features.  Also called lag or lookback.\n",
    "\n",
    "#### nb_input_series: \n",
    "The number of input timeseries; 1 for a single timeseries.\n",
    "The `X` input to ``fit()`` should be an array of shape ``(n_instances, window_size, nb_input_series)``; each instance is a 2D array of shape ``(window_size, nb_input_series)``. For example, for `window_size` = 3 and `nb_input_series` = 1 (a single timeseries), one instance could be ``[[0], [1], [2]]``. See ``make_timeseries_instances()``.\n",
    "\n",
    "#### nb_outputs: \n",
    "The output dimension, often equal to the number of inputs. For each input instance (array with shape ``(window_size, nb_input_series)``), the output is a vector of size `nb_outputs`, usually the value(s) predicted to come after the last value in that input instance, i.e., the next value in the sequence. The `y` input to ``fit()`` should be an array of shape ``(n_instances, nb_outputs)``.\n",
    "\n",
    "#### filter_length: \n",
    "the size (along the `window_size` dimension) of the sliding window that gets convolved with each position along each instance. The difference between 1D and 2D convolution is that a 1D filter's \"height\" is fixed\n",
    "      to the number of input timeseries (its \"width\" being `filter_length`), and it can only slide along the window dimension. This is useful as generally the input timeseries have no spatial/ordinal relationship, so it's not meaningful to look for patterns that are invariant with respect to subsets of the timeseries.\n",
    "\n",
    "#### nb_filters: \n",
    "The number of different filters to learn (roughly, input patterns to recognize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_timeseries_regressor(window_size, filter_length, nb_input_series=1, nb_outputs=1, nb_filter=4):\n",
    "    \n",
    "    model = Sequential((\n",
    "        # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "        # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "        # the input timeseries, the activation of each filter at that position.\n",
    "        \n",
    "        Conv1D(filters=nb_filter, kernel_size=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),\n",
    "        MaxPooling1D(), # Downsample the output of convolution by 2X.\n",
    "        Conv1D(filters=nb_filter, kernel_size=filter_length, activation='relu'),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(nb_outputs, activation='linear'), # For binary classification, change the activation to 'sigmoid'\n",
    "    ))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    # To perform (binary) classification instead:\n",
    "    # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 60\n",
    "nb_filter = 4\n",
    "filter_length = 5\n",
    "nb_input_series = 2\n",
    "nb_outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model with input size (None, 60, 2), output size (None, 2), 4 conv filters of length 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 56, 4)             44        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 28, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 24, 4)             84        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 12, 4)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 226\n",
      "Trainable params: 226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_timeseries_regressor(window_size=window_size, filter_length=filter_length, nb_input_series=nb_input_series, nb_outputs=nb_outputs, nb_filter=nb_filter)\n",
    "print('\\n\\nModel with input size {}, output size {}, {} conv filters of length {}'.format(model.input_shape, model.output_shape, nb_filter, filter_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
